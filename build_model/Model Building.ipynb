{"cells":[{"cell_type":"code","source":["#https://towardsdatascience.com/how-to-train-an-image-classifier-in-pytorch-and-use-it-to-perform-basic-inference-on-single-images-99465a1e9bf5\n#https://www.learnopencv.com/image-classification-using-transfer-learning-in-pytorch/\n#https://github.com/spmallick/learnopencv/blob/master/Image-Classification-in-PyTorch/image_classification_using_transfer_learning_in_pytorch.ipynb\n\n#Web: https://medium.com/@mohcufe/how-to-deploy-your-trained-pytorch-model-on-heroku-ff4b73085ddd\n\nimport torch\nfrom torch import nn\nfrom torch import optim\nimport torch.nn.functional as F\nfrom collections import OrderedDict\nfrom PIL import Image\nfrom torchvision import datasets, models, transforms\nfrom torch.utils.data import DataLoader\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport time"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["%sh\ncp /dbfs/mnt/RAW/FILES/SYNAPSE/POC/UDACITY/PRODUCT_IMAGES_CLASSIFIER_DATA_SPLIT.ZIP /local_disk0/tmp/\nunzip /local_disk0/tmp/PRODUCT_IMAGES_CLASSIFIER_DATA_SPLIT.ZIP -d /local_disk0/tmp/\n"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["%sh\n# ls /dbfs/mnt/RAW/FILES/SYNAPSE/POC/UDACITY/PRODUCT_IMAGES_CLASSIFIER/\n# mv /dbfs/mnt/RAW/FILES/SYNAPSE/POC/UDACITY/PRODUCT_IMAGES_CLASSIFIER/*.pth /dbfs/mnt/RAW/FILES/SYNAPSE/POC/UDACITY/PRODUCT_IMAGES_CLASSIFIER_RESNET152-75EPOCHS/\n# mv /dbfs/mnt/RAW/FILES/SYNAPSE/POC/UDACITY/PRODUCT_IMAGES_CLASSIFIER/*.png /dbfs/mnt/RAW/FILES/SYNAPSE/POC/UDACITY/PRODUCT_IMAGES_CLASSIFIER_RESNET152-75EPOCHS/\n# rm /dbfs/mnt/RAW/FILES/SYNAPSE/POC/UDACITY/PRODUCT_IMAGES_CLASSIFIER/*.pth"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["image_dir = '/local_disk0/tmp/'\nimage_dir_train = image_dir + 'TRAIN/'\nimage_dir_test = image_dir + 'TEST/'\nimage_dir_valid = image_dir + 'VALID/'"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["image_transforms = { \n    'train': transforms.Compose([\n        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n        transforms.RandomRotation(degrees=15),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomRotation(degrees=15),\n        transforms.ColorJitter(),\n        transforms.CenterCrop(size=224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'valid': transforms.Compose([\n        transforms.Resize(size=256),\n        transforms.CenterCrop(size=224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'test': transforms.Compose([\n        transforms.Resize(size=256),\n        transforms.CenterCrop(size=224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n    ])\n}"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"code","source":["# Load the Data\n \n# Batch size\nbatch_size = 32\n \n# Number of classes\nnum_classes = len(os.listdir(image_dir_train))\n \n# Load Data from folders\ndata = {\n    'train': datasets.ImageFolder(root=image_dir_train, transform=image_transforms['train']),\n    'valid': datasets.ImageFolder(root=image_dir_valid, transform=image_transforms['valid']),\n    'test': datasets.ImageFolder(root=image_dir_test, transform=image_transforms['test'])\n}\n \n# Size of Data, to be used for calculating Average Loss and Accuracy\ntrain_data_size = len(data['train'])\nvalid_data_size = len(data['valid'])\ntest_data_size = len(data['test'])\n \n# Create iterators for the Data loaded using DataLoader module\ntrain_data_loader = DataLoader(data['train'], batch_size=batch_size, shuffle=True)\nvalid_data_loader = DataLoader(data['valid'], batch_size=batch_size, shuffle=True)\ntest_data_loader = DataLoader(data['test'], batch_size=batch_size, shuffle=True)\n \n# Print Statistics\nprint('Data Set Sizes - Train: {} Valid: {}, Test: {}'.format(train_data_size, valid_data_size, test_data_size))\nprint('Number of Classes: {}'.format(num_classes))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Data Set Sizes - Train: 18494 Valid: 2311, Test: 2350\nNumber of Classes: 73\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["data['train'].class_to_idx"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[49]: {&#39;10108664&#39;: 0,\n &#39;10109264&#39;: 1,\n &#39;10109266&#39;: 2,\n &#39;10109268&#39;: 3,\n &#39;10109272&#39;: 4,\n &#39;10109275&#39;: 5,\n &#39;10118456&#39;: 6,\n &#39;10119951&#39;: 7,\n &#39;10119954&#39;: 8,\n &#39;10119956&#39;: 9,\n &#39;10119963&#39;: 10,\n &#39;10120054&#39;: 11,\n &#39;10121868&#39;: 12,\n &#39;10121870&#39;: 13,\n &#39;10121871&#39;: 14,\n &#39;10121872&#39;: 15,\n &#39;10121874&#39;: 16,\n &#39;10129343&#39;: 17,\n &#39;10129365&#39;: 18,\n &#39;10137237&#39;: 19,\n &#39;10137240&#39;: 20,\n &#39;10141489&#39;: 21,\n &#39;10141490&#39;: 22,\n &#39;10141492&#39;: 23,\n &#39;10141493&#39;: 24,\n &#39;10141494&#39;: 25,\n &#39;10142529&#39;: 26,\n &#39;10142531&#39;: 27,\n &#39;10142631&#39;: 28,\n &#39;10142633&#39;: 29,\n &#39;10142635&#39;: 30,\n &#39;10142637&#39;: 31,\n &#39;10142638&#39;: 32,\n &#39;10159239&#39;: 33,\n &#39;10172358&#39;: 34,\n &#39;10172361&#39;: 35,\n &#39;10172367&#39;: 36,\n &#39;10176723&#39;: 37,\n &#39;10176726&#39;: 38,\n &#39;10176727&#39;: 39,\n &#39;10176739&#39;: 40,\n &#39;10176743&#39;: 41,\n &#39;10176755&#39;: 42,\n &#39;10176763&#39;: 43,\n &#39;10176807&#39;: 44,\n &#39;10176814&#39;: 45,\n &#39;10191680&#39;: 46,\n &#39;10192085&#39;: 47,\n &#39;10192091&#39;: 48,\n &#39;10192353&#39;: 49,\n &#39;10192356&#39;: 50,\n &#39;10192359&#39;: 51,\n &#39;10192365&#39;: 52,\n &#39;10192375&#39;: 53,\n &#39;1019239&#39;: 54,\n &#39;10192390&#39;: 55,\n &#39;10193235&#39;: 56,\n &#39;10193236&#39;: 57,\n &#39;10193238&#39;: 58,\n &#39;10193239&#39;: 59,\n &#39;10193240&#39;: 60,\n &#39;10193241&#39;: 61,\n &#39;10193253&#39;: 62,\n &#39;10193255&#39;: 63,\n &#39;10193256&#39;: 64,\n &#39;10197121&#39;: 65,\n &#39;10197122&#39;: 66,\n &#39;10197124&#39;: 67,\n &#39;10197126&#39;: 68,\n &#39;10197127&#39;: 69,\n &#39;10197128&#39;: 70,\n &#39;1019713&#39;: 71,\n &#39;10202946&#39;: 72}</div>"]}}],"execution_count":7},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n#model = models.densenet121(pretrained=True)\nmodel = models.resnet50(pretrained=True)\n#model = models.resnet152(pretrained=True)\n#model = models.inception_v3(pretrained=True)\n\nmodel_name = model.__class__.__name__\nprint('Device: {}'.format(device))\nprint('ModelName: {}'.format(model_name))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Device: cuda\nModelName: ResNet\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["print(model)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (4): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (5): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n)\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["for param in model.parameters():\n    param.require_grad = False\n\nif model_name == 'ResNet':\n  fc_inputs = model.fc.in_features\n\nif model_name == 'DenseNet':\n  fc_inputs = 1024\n\nif model_name == 'Inception3':\n  fc_inputs = model.fc.in_features\n  \nmodel.fc = nn.Sequential(\n    nn.Linear(fc_inputs, 256),\n    nn.ReLU(),\n    nn.Dropout(0.4),\n    nn.Linear(256, num_classes), # Since 10 possible outputs\n    nn.LogSoftmax(dim=1) # For using NLLLoss()\n)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"code","source":["optimizer = optim.Adam(model.parameters())\nloss_criterion = nn.NLLLoss()\n\nmodel.to(device)\n\n##NOTE THAT DATAPARALLEL CREATES SOME CHALLENGES WHEN CONSUMING THE SAVED MODEL. NOT USING THIS FOR NOW.\n#model = nn.DataParallel(model)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[54]: ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (4): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (5): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Sequential(\n    (0): Linear(in_features=2048, out_features=256, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.4, inplace=False)\n    (3): Linear(in_features=256, out_features=73, bias=True)\n    (4): LogSoftmax()\n  )\n)</div>"]}}],"execution_count":11},{"cell_type":"code","source":["def train_and_validate(model, loss_criterion, optimizer, start_epoch=1, epochs=25):\n  '''\n  Function to train and validate\n  Parameters\n      :param model: Model to train and validate\n      :param loss_criterion: Loss Criterion to minimize\n      :param optimizer: Optimizer for computing gradients\n      :param epochs: Number of epochs (default=25)\n\n  Returns\n      model: Trained Model with best validation accuracy\n      history: (dict object): Having training loss, accuracy and validation loss, accuracy\n  '''\n\n  start = time.time()\n  history = []\n  best_acc = 0.0\n\n  for epoch in range(start_epoch, epochs):\n      epoch_start = time.time()\n      print(\"Epoch: {}/{}\".format(epoch, epochs))\n\n      # Set to training mode\n      model.train()\n\n      # Loss and Accuracy within the epoch\n      train_loss = 0.0\n      train_acc = 0.0\n\n      valid_loss = 0.0\n      valid_acc = 0.0\n\n      for i, (inputs, labels) in enumerate(train_data_loader):\n\n          inputs = inputs.to(device)\n          labels = labels.to(device)\n\n          # Clean existing gradients\n          optimizer.zero_grad()\n\n          # Forward pass - compute outputs on input data using the model\n          outputs = model(inputs)\n\n          # Compute loss\n          loss = loss_criterion(outputs, labels)\n\n          # Backpropagate the gradients\n          loss.backward()\n\n          # Update the parameters\n          optimizer.step()\n\n          # Compute the total loss for the batch and add it to train_loss\n          train_loss += loss.item() * inputs.size(0)\n\n          # Compute the accuracy\n          ret, predictions = torch.max(outputs.data, 1)\n          correct_counts = predictions.eq(labels.data.view_as(predictions))\n\n          # Convert correct_counts to float and then compute the mean\n          acc = torch.mean(correct_counts.type(torch.FloatTensor))\n\n          # Compute total accuracy in the whole batch and add to train_acc\n          train_acc += acc.item() * inputs.size(0)\n\n          print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item()))\n\n\n      # Validation - No gradient tracking needed\n      with torch.no_grad():\n\n          # Set to evaluation mode\n          model.eval()\n\n          # Validation loop\n          for j, (inputs, labels) in enumerate(valid_data_loader):\n              inputs = inputs.to(device)\n              labels = labels.to(device)\n\n              # Forward pass - compute outputs on input data using the model\n              outputs = model(inputs)\n\n              # Compute loss\n              loss = loss_criterion(outputs, labels)\n\n              # Compute the total loss for the batch and add it to valid_loss\n              valid_loss += loss.item() * inputs.size(0)\n\n              # Calculate validation accuracy\n              ret, predictions = torch.max(outputs.data, 1)\n              correct_counts = predictions.eq(labels.data.view_as(predictions))\n\n              # Convert correct_counts to float and then compute the mean\n              acc = torch.mean(correct_counts.type(torch.FloatTensor))\n\n              # Compute total accuracy in the whole batch and add to valid_acc\n              valid_acc += acc.item() * inputs.size(0)\n\n              print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n\n      # Find average training loss and training accuracy\n      avg_train_loss = train_loss/train_data_size \n      avg_train_acc = train_acc/train_data_size\n\n      # Find average training loss and training accuracy\n      avg_valid_loss = valid_loss/valid_data_size \n      avg_valid_acc = valid_acc/valid_data_size\n\n      history.append([avg_train_loss, avg_valid_loss, avg_train_acc, avg_valid_acc])\n\n      epoch_end = time.time()\n\n      print(\"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \\n\\t\\tValidation : Loss : {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s\".format(epoch, avg_train_loss, avg_train_acc*100, avg_valid_loss, avg_valid_acc*100, epoch_end-epoch_start))\n\n      # Save the model\n      #torch.save(model, '/dbfs/mnt/RAW/FILES/SYNAPSE/POC/UDACITY/PRODUCT_IMAGES_CLASSIFIER/checkpoint-' +str(epoch) + '.pth')\n      torch.save({\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'loss': loss,\n        'class_to_idx': data['train'].class_to_idx,\n      }, '/dbfs/mnt/RAW/FILES/SYNAPSE/POC/UDACITY/PRODUCT_IMAGES_CLASSIFIER/checkpoint-' +str(epoch) + '.pth')\n\n  return model, history"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":12},{"cell_type":"code","source":["num_epochs = 20\nstart_epoch = 1\ntrained_model, history = train_and_validate(model, loss_criterion, optimizer, start_epoch, num_epochs)\n\ntorch.save(history, '/dbfs/mnt/RAW/FILES/SYNAPSE/POC/UDACITY/PRODUCT_IMAGES_CLASSIFIER/history.pth')\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Epoch: 1/20\nBatch number: 000, Training: Loss: 4.2899, Accuracy: 0.0000\nBatch number: 001, Training: Loss: 4.2997, Accuracy: 0.0312\nBatch number: 002, Training: Loss: 4.3270, Accuracy: 0.0312\nBatch number: 003, Training: Loss: 4.4710, Accuracy: 0.0312\nBatch number: 004, Training: Loss: 4.1706, Accuracy: 0.0625\nBatch number: 005, Training: Loss: 4.5966, Accuracy: 0.0000\nBatch number: 006, Training: Loss: 4.0576, Accuracy: 0.0625\nBatch number: 007, Training: Loss: 4.2916, Accuracy: 0.0000\nBatch number: 008, Training: Loss: 3.9127, Accuracy: 0.0625\nBatch number: 009, Training: Loss: 4.1740, Accuracy: 0.0625\nBatch number: 010, Training: Loss: 4.0863, Accuracy: 0.0312\nBatch number: 011, Training: Loss: 4.2559, Accuracy: 0.0312\nBatch number: 012, Training: Loss: 3.8568, Accuracy: 0.0625\nBatch number: 013, Training: Loss: 4.0844, Accuracy: 0.0000\nBatch number: 014, Training: Loss: 3.8096, Accuracy: 0.0625\nBatch number: 015, Training: Loss: 4.0402, Accuracy: 0.0625\nBatch number: 016, Training: Loss: 4.0031, Accuracy: 0.0312\nBatch number: 017, Training: Loss: 3.7987, Accuracy: 0.0625\nBatch number: 018, Training: Loss: 3.7115, Accuracy: 0.1562\nBatch number: 019, Training: Loss: 3.9446, Accuracy: 0.1250\nBatch number: 020, Training: Loss: 3.5277, Accuracy: 0.1562\nBatch number: 021, Training: Loss: 3.9451, Accuracy: 0.1250\nBatch number: 022, Training: Loss: 3.6256, Accuracy: 0.0625\nBatch number: 023, Training: Loss: 3.7227, Accuracy: 0.0938\nBatch number: 024, Training: Loss: 3.7926, Accuracy: 0.0625\nBatch number: 025, Training: Loss: 4.1130, Accuracy: 0.0938\nBatch number: 026, Training: Loss: 3.8647, Accuracy: 0.0625\nBatch number: 027, Training: Loss: 3.6832, Accuracy: 0.0625\nBatch number: 028, Training: Loss: 3.5173, Accuracy: 0.1250\nBatch number: 029, Training: Loss: 3.3986, Accuracy: 0.0938\nBatch number: 030, Training: Loss: 4.1647, Accuracy: 0.0312\nBatch number: 031, Training: Loss: 3.5290, Accuracy: 0.1562\nBatch number: 032, Training: Loss: 3.8913, Accuracy: 0.1250\nBatch number: 033, Training: Loss: 3.5447, Accuracy: 0.0938\nBatch number: 034, Training: Loss: 3.4714, Accuracy: 0.1562\nBatch number: 035, Training: Loss: 3.4110, Accuracy: 0.0938\nBatch number: 036, Training: Loss: 3.8121, Accuracy: 0.0625\nBatch number: 037, Training: Loss: 3.4884, Accuracy: 0.1875\nBatch number: 038, Training: Loss: 3.6870, Accuracy: 0.0938\nBatch number: 039, Training: Loss: 3.3677, Accuracy: 0.0625\nBatch number: 040, Training: Loss: 3.5169, Accuracy: 0.0625\nBatch number: 041, Training: Loss: 3.3415, Accuracy: 0.1875\nBatch number: 042, Training: Loss: 3.8463, Accuracy: 0.0000\nBatch number: 043, Training: Loss: 3.6403, Accuracy: 0.1250\nBatch number: 044, Training: Loss: 3.4727, Accuracy: 0.1250\nBatch number: 045, Training: Loss: 2.9434, Accuracy: 0.2188\nBatch number: 046, Training: Loss: 3.4400, Accuracy: 0.0938\nBatch number: 047, Training: Loss: 3.3613, Accuracy: 0.1250\nBatch number: 048, Training: Loss: 3.5991, Accuracy: 0.1250\nBatch number: 049, Training: Loss: 3.2089, Accuracy: 0.1250\nBatch number: 050, Training: Loss: 3.3108, Accuracy: 0.2500\nBatch number: 051, Training: Loss: 3.7982, Accuracy: 0.0000\nBatch number: 052, Training: Loss: 3.0284, Accuracy: 0.1250\nBatch number: 053, Training: Loss: 3.2295, Accuracy: 0.1562\nBatch number: 054, Training: Loss: 3.3982, Accuracy: 0.1250\nBatch number: 055, Training: Loss: 3.1671, Accuracy: 0.1875\nBatch number: 056, Training: Loss: 3.2780, Accuracy: 0.1875\nBatch number: 057, Training: Loss: 3.6020, Accuracy: 0.1250\nBatch number: 058, Training: Loss: 2.9088, Accuracy: 0.1562\nBatch number: 059, Training: Loss: 3.4525, Accuracy: 0.0938\nBatch number: 060, Training: Loss: 3.1566, Accuracy: 0.1875\nBatch number: 061, Training: Loss: 3.5877, Accuracy: 0.0938\nBatch number: 062, Training: Loss: 3.9708, Accuracy: 0.1562\nBatch number: 063, Training: Loss: 3.3386, Accuracy: 0.0938\nBatch number: 064, Training: Loss: 3.2660, Accuracy: 0.1875\nBatch number: 065, Training: Loss: 3.1489, Accuracy: 0.2188\nBatch number: 066, Training: Loss: 3.0297, Accuracy: 0.1250\nBatch number: 067, Training: Loss: 2.8379, Accuracy: 0.1875\nBatch number: 068, Training: Loss: 3.3460, Accuracy: 0.1250\nBatch number: 069, Training: Loss: 3.4901, Accuracy: 0.1562\nBatch number: 070, Training: Loss: 3.1708, Accuracy: 0.1875\nBatch number: 071, Training: Loss: 3.1333, Accuracy: 0.1875\nBatch number: 072, Training: Loss: 2.9754, Accuracy: 0.1562\nBatch number: 073, Training: Loss: 3.5565, Accuracy: 0.1875\nBatch number: 074, Training: Loss: 3.2769, Accuracy: 0.2812\nBatch number: 075, Training: Loss: 2.7727, Accuracy: 0.1250\nBatch number: 076, Training: Loss: 3.1859, Accuracy: 0.1875\nBatch number: 077, Training: Loss: 3.2426, Accuracy: 0.2188\nBatch number: 078, Training: Loss: 2.9861, Accuracy: 0.1562\nBatch number: 079, Training: Loss: 2.9519, Accuracy: 0.1875\nBatch number: 080, Training: Loss: 2.9470, Accuracy: 0.1562\nBatch number: 081, Training: Loss: 3.2040, Accuracy: 0.0938\nBatch number: 082, Training: Loss: 2.7592, Accuracy: 0.1562\nBatch number: 083, Training: Loss: 3.1158, Accuracy: 0.1562\nBatch number: 084, Training: Loss: 2.9320, Accuracy: 0.2188\nBatch number: 085, Training: Loss: 2.6970, Accuracy: 0.2812\nBatch number: 086, Training: Loss: 2.9809, Accuracy: 0.1562\nBatch number: 087, Training: Loss: 2.9740, Accuracy: 0.1875\nBatch number: 088, Training: Loss: 2.6820, Accuracy: 0.2188\nBatch number: 089, Training: Loss: 2.8017, Accuracy: 0.2500\nBatch number: 090, Training: Loss: 2.5670, Accuracy: 0.3125\nBatch number: 091, Training: Loss: 3.1761, Accuracy: 0.0625\nBatch number: 092, Training: Loss: 3.0544, Accuracy: 0.3125\nBatch number: 093, Training: Loss: 2.9487, Accuracy: 0.1562\nBatch number: 094, Training: Loss: 2.9799, Accuracy: 0.1250\nBatch number: 095, Training: Loss: 2.7048, Accuracy: 0.2500\nBatch number: 096, Training: Loss: 2.4628, Accuracy: 0.3438\nBatch number: 097, Training: Loss: 2.5096, Accuracy: 0.2812\nBatch number: 098, Training: Loss: 2.3245, Accuracy: 0.3750\nBatch number: 099, Training: Loss: 2.6067, Accuracy: 0.3438\nBatch number: 100, Training: Loss: 2.5302, Accuracy: 0.3125\nBatch number: 101, Training: Loss: 3.2645, Accuracy: 0.1250\nBatch number: 102, Training: Loss: 2.5783, Accuracy: 0.3125\nBatch number: 103, Training: Loss: 2.6076, Accuracy: 0.2812\nBatch number: 104, Training: Loss: 2.5440, Accuracy: 0.2500\nBatch number: 105, Training: Loss: 2.5244, Accuracy: 0.4688\nBatch number: 106, Training: Loss: 2.5271, Accuracy: 0.3125\nBatch number: 107, Training: Loss: 3.0718, Accuracy: 0.1562\nBatch number: 108, Training: Loss: 2.6763, Accuracy: 0.2500\nBatch number: 109, Training: Loss: 2.6465, Accuracy: 0.2188\nBatch number: 110, Training: Loss: 2.7155, Accuracy: 0.2500\nBatch number: 111, Training: Loss: 2.5388, Accuracy: 0.2812\nBatch number: 112, Training: Loss: 2.3558, Accuracy: 0.3438\nBatch number: 113, Training: Loss: 2.5760, Accuracy: 0.2812\nBatch number: 114, Training: Loss: 2.5893, Accuracy: 0.2812\nBatch number: 115, Training: Loss: 2.6952, Accuracy: 0.2188\nBatch number: 116, Training: Loss: 2.7542, Accuracy: 0.2188\nBatch number: 117, Training: Loss: 2.6242, Accuracy: 0.2500\nBatch number: 118, Training: Loss: 3.2924, Accuracy: 0.1875\nBatch number: 119, Training: Loss: 2.4284, Accuracy: 0.1875\nBatch number: 120, Training: Loss: 2.4421, Accuracy: 0.2188\nBatch number: 121, Training: Loss: 2.8827, Accuracy: 0.2500\nBatch number: 122, Training: Loss: 2.4872, Accuracy: 0.2188\nBatch number: 123, Training: Loss: 2.1028, Accuracy: 0.4375\nBatch number: 124, Training: Loss: 2.4550, Accuracy: 0.2812\nBatch number: 125, Training: Loss: 2.5590, Accuracy: 0.2812\nBatch number: 126, Training: Loss: 2.2993, Accuracy: 0.2812\nBatch number: 127, Training: Loss: 3.1505, Accuracy: 0.2500\nBatch number: 128, Training: Loss: 2.6059, Accuracy: 0.3750\nBatch number: 129, Training: Loss: 2.6340, Accuracy: 0.2188\nBatch number: 130, Training: Loss: 2.5714, Accuracy: 0.2500\nBatch number: 131, Training: Loss: 2.1400, Accuracy: 0.3125\nBatch number: 132, Training: Loss: 2.4866, Accuracy: 0.2500\nBatch number: 133, Training: Loss: 2.2619, Accuracy: 0.3438\nBatch number: 134, Training: Loss: 2.4416, Accuracy: 0.3125\nBatch number: 135, Training: Loss: 2.3729, Accuracy: 0.2500\nBatch number: 136, Training: Loss: 2.6715, Accuracy: 0.3125\nBatch number: 137, Training: Loss: 2.6113, Accuracy: 0.1562\nBatch number: 138, Training: Loss: 2.6671, Accuracy: 0.2188\nBatch number: 139, Training: Loss: 2.4505, Accuracy: 0.2188\nBatch number: 140, Training: Loss: 2.8053, Accuracy: 0.2188\nBatch number: 141, Training: Loss: 2.7177, Accuracy: 0.1562\nBatch number: 142, Training: Loss: 2.6092, Accuracy: 0.1562\nBatch number: 143, Training: Loss: 2.3577, Accuracy: 0.2812\nBatch number: 144, Training: Loss: 2.8176, Accuracy: 0.2188\nBatch number: 145, Training: Loss: 2.3391, Accuracy: 0.4062\nBatch number: 146, Training: Loss: 2.4488, Accuracy: 0.3125\nBatch number: 147, Training: Loss: 2.5397, Accuracy: 0.1250\nBatch number: 148, Training: Loss: 2.6974, Accuracy: 0.2188\nBatch number: 149, Training: Loss: 2.4665, Accuracy: 0.3125\nBatch number: 150, Training: Loss: 2.8987, Accuracy: 0.3438\nBatch number: 151, Training: Loss: 2.7439, Accuracy: 0.2812\nBatch number: 152, Training: Loss: 2.3955, Accuracy: 0.3125\nBatch number: 153, Training: Loss: 2.3958, Accuracy: 0.2812\nBatch number: 154, Training: Loss: 3.2870, Accuracy: 0.1250\nBatch number: 155, Training: Loss: 2.6881, Accuracy: 0.2500\nBatch number: 156, Training: Loss: 2.1486, Accuracy: 0.3750\nBatch number: 157, Training: Loss: 2.9353, Accuracy: 0.1875\nBatch number: 158, Training: Loss: 2.3290, Accuracy: 0.3125\nBatch number: 159, Training: Loss: 2.6242, Accuracy: 0.3438\nBatch number: 160, Training: Loss: 2.3714, Accuracy: 0.2188\nBatch number: 161, Training: Loss: 2.6809, Accuracy: 0.3438\nBatch number: 162, Training: Loss: 2.4042, Accuracy: 0.3438\nBatch number: 163, Training: Loss: 2.6028, Accuracy: 0.2188\nBatch number: 164, Training: Loss: 2.1476, Accuracy: 0.3125\nBatch number: 165, Training: Loss: 2.5061, Accuracy: 0.3438\nBatch number: 166, Training: Loss: 2.6806, Accuracy: 0.1562\nBatch number: 167, Training: Loss: 1.9896, Accuracy: 0.4062\nBatch number: 168, Training: Loss: 2.3470, Accuracy: 0.2812\nBatch number: 169, Training: Loss: 2.2607, Accuracy: 0.3125\nBatch number: 170, Training: Loss: 2.1989, Accuracy: 0.4375\nBatch number: 171, Training: Loss: 1.7436, Accuracy: 0.4062\nBatch number: 172, Training: Loss: 3.0266, Accuracy: 0.1875\nBatch number: 173, Training: Loss: 2.6172, Accuracy: 0.1875\nBatch number: 174, Training: Loss: 3.0303, Accuracy: 0.0938\nBatch number: 175, Training: Loss: 2.1989, Accuracy: 0.4375\nBatch number: 176, Training: Loss: 2.1910, Accuracy: 0.2500\nBatch number: 177, Training: Loss: 2.4503, Accuracy: 0.2812\nBatch number: 178, Training: Loss: 2.8140, Accuracy: 0.2500\nBatch number: 179, Training: Loss: 2.1381, Accuracy: 0.3438\nBatch number: 180, Training: Loss: 2.5880, Accuracy: 0.3125\nBatch number: 181, Training: Loss: 2.2509, Accuracy: 0.3438\nBatch number: 182, Training: Loss: 2.3621, Accuracy: 0.2812\nBatch number: 183, Training: Loss: 2.4544, Accuracy: 0.2500\nBatch number: 184, Training: Loss: 2.4701, Accuracy: 0.3125\nBatch number: 185, Training: Loss: 2.2103, Accuracy: 0.3438\nBatch number: 186, Training: Loss: 2.4576, Accuracy: 0.1875\nBatch number: 187, Training: Loss: 2.5331, Accuracy: 0.2500\nBatch number: 188, Training: Loss: 1.8445, Accuracy: 0.3438\nBatch number: 189, Training: Loss: 2.6103, Accuracy: 0.1875\nBatch number: 190, Training: Loss: 2.6035, Accuracy: 0.3125\nBatch number: 191, Training: Loss: 2.1429, Accuracy: 0.4688\nBatch number: 192, Training: Loss: 2.1188, Accuracy: 0.4062\nBatch number: 193, Training: Loss: 2.7887, Accuracy: 0.1250\nBatch number: 194, Training: Loss: 2.5719, Accuracy: 0.3125\nBatch number: 195, Training: Loss: 2.0984, Accuracy: 0.3438\nBatch number: 196, Training: Loss: 1.8734, Accuracy: 0.4688\nBatch number: 197, Training: Loss: 2.3161, Accuracy: 0.2500\nBatch number: 198, Training: Loss: 2.1157, Accuracy: 0.3438\nBatch number: 199, Training: Loss: 2.2986, Accuracy: 0.3438\nBatch number: 200, Training: Loss: 2.9147, Accuracy: 0.2188\nBatch number: 201, Training: Loss: 1.8311, Accuracy: 0.3438\nBatch number: 202, Training: Loss: 2.2019, Accuracy: 0.2812\nBatch number: 203, Training: Loss: 2.3869, Accuracy: 0.1875\nBatch number: 204, Training: Loss: 1.9660, Accuracy: 0.2500\nBatch number: 205, Training: Loss: 1.9634, Accuracy: 0.4375\nBatch number: 206, Training: Loss: 2.2259, Accuracy: 0.3750\nBatch number: 207, Training: Loss: 2.5976, Accuracy: 0.3125\nBatch number: 208, Training: Loss: 2.1631, Accuracy: 0.3438\nBatch number: 209, Training: Loss: 1.8436, Accuracy: 0.4375\nBatch number: 210, Training: Loss: 2.3146, Accuracy: 0.2500\nBatch number: 211, Training: Loss: 2.3134, Accuracy: 0.3125\nBatch number: 212, Training: Loss: 2.2192, Accuracy: 0.2812\nBatch number: 213, Training: Loss: 2.2143, Accuracy: 0.2812\nBatch number: 214, Training: Loss: 2.2229, Accuracy: 0.2812\nBatch number: 215, Training: Loss: 2.0154, Accuracy: 0.2812\nBatch number: 216, Training: Loss: 2.6927, Accuracy: 0.2500\nBatch number: 217, Training: Loss: 2.0040, Accuracy: 0.4375\nBatch number: 218, Training: Loss: 1.8297, Accuracy: 0.3438\nBatch number: 219, Training: Loss: 1.7588, Accuracy: 0.4375\nBatch number: 220, Training: Loss: 2.4908, Accuracy: 0.2500\nBatch number: 221, Training: Loss: 2.1578, Accuracy: 0.3750\nBatch number: 222, Training: Loss: 1.9132, Accuracy: 0.4375\nBatch number: 223, Training: Loss: 2.3379, Accuracy: 0.3125\nBatch number: 224, Training: Loss: 1.9608, Accuracy: 0.3438\nBatch number: 225, Training: Loss: 2.2007, Accuracy: 0.3125\nBatch number: 226, Training: Loss: 1.8144, Accuracy: 0.4375\nBatch number: 227, Training: Loss: 2.2831, Accuracy: 0.3125\nBatch number: 228, Training: Loss: 1.6810, Accuracy: 0.4062\nBatch number: 229, Training: Loss: 1.8855, Accuracy: 0.4375\nBatch number: 230, Training: Loss: 2.3413, Accuracy: 0.2188\nBatch number: 231, Training: Loss: 2.1004, Accuracy: 0.3125\nBatch number: 232, Training: Loss: 2.3916, Accuracy: 0.2812\nBatch number: 233, Training: Loss: 1.9691, Accuracy: 0.4062\nBatch number: 234, Training: Loss: 1.7077, Accuracy: 0.4688\nBatch number: 235, Training: Loss: 2.9533, Accuracy: 0.3750\nBatch number: 236, Training: Loss: 2.6585, Accuracy: 0.1875\nBatch number: 237, Training: Loss: 2.0225, Accuracy: 0.3750\nBatch number: 238, Training: Loss: 2.2422, Accuracy: 0.3438\nBatch number: 239, Training: Loss: 2.1128, Accuracy: 0.3125\nBatch number: 240, Training: Loss: 1.7497, Accuracy: 0.5625\nBatch number: 241, Training: Loss: 2.3222, Accuracy: 0.2500\nBatch number: 242, Training: Loss: 2.1382, Accuracy: 0.3438\nBatch number: 243, Training: Loss: 2.2447, Accuracy: 0.2812\nBatch number: 244, Training: Loss: 2.0794, Accuracy: 0.3750\nBatch number: 245, Training: Loss: 2.3106, Accuracy: 0.2812\nBatch number: 246, Training: Loss: 1.9695, Accuracy: 0.4062\nBatch number: 247, Training: Loss: 2.0545, Accuracy: 0.3438\nBatch number: 248, Training: Loss: 2.4015, Accuracy: 0.3438\nBatch number: 249, Training: Loss: 2.1590, Accuracy: 0.3438\nBatch number: 250, Training: Loss: 2.8157, Accuracy: 0.1562\nBatch number: 251, Training: Loss: 2.2195, Accuracy: 0.3750\nBatch number: 252, Training: Loss: 1.9079, Accuracy: 0.3438\nBatch number: 253, Training: Loss: 2.1602, Accuracy: 0.2812\nBatch number: 254, Training: Loss: 2.1362, Accuracy: 0.3750\nBatch number: 255, Training: Loss: 2.0840, Accuracy: 0.4375\nBatch number: 256, Training: Loss: 2.1633, Accuracy: 0.4375\nBatch number: 257, Training: Loss: 2.5004, Accuracy: 0.2188\nBatch number: 258, Training: Loss: 1.7204, Accuracy: 0.3750\nBatch number: 259, Training: Loss: 1.9306, Accuracy: 0.3750\nBatch number: 260, Training: Loss: 2.1962, Accuracy: 0.3125\nBatch number: 261, Training: Loss: 2.3801, Accuracy: 0.2188\nBatch number: 262, Training: Loss: 1.6003, Accuracy: 0.4062\nBatch number: 263, Training: Loss: 2.1679, Accuracy: 0.3438\nBatch number: 264, Training: Loss: 1.7283, Accuracy: 0.4062\nBatch number: 265, Training: Loss: 1.8947, Accuracy: 0.3438\nBatch number: 266, Training: Loss: 2.4238, Accuracy: 0.2812\nBatch number: 267, Training: Loss: 1.4127, Accuracy: 0.5000\nBatch number: 268, Training: Loss: 1.7673, Accuracy: 0.3750\nBatch number: 269, Training: Loss: 2.5947, Accuracy: 0.1875\nBatch number: 270, Training: Loss: 2.3985, Accuracy: 0.2500\nBatch number: 271, Training: Loss: 1.9138, Accuracy: 0.5000\nBatch number: 272, Training: Loss: 2.3869, Accuracy: 0.2500\nBatch number: 273, Training: Loss: 1.9512, Accuracy: 0.3750\nBatch number: 274, Training: Loss: 2.0689, Accuracy: 0.3750\nBatch number: 275, Training: Loss: 1.7658, Accuracy: 0.3125\nBatch number: 276, Training: Loss: 1.9720, Accuracy: 0.4375\nBatch number: 277, Training: Loss: 2.0961, Accuracy: 0.3750\nBatch number: 278, Training: Loss: 2.1599, Accuracy: 0.3750\nBatch number: 279, Training: Loss: 1.9273, Accuracy: 0.3750\nBatch number: 280, Training: Loss: 1.7541, Accuracy: 0.4375\nBatch number: 281, Training: Loss: 1.6707, Accuracy: 0.4375\nBatch number: 282, Training: Loss: 1.8544, Accuracy: 0.5312\nBatch number: 283, Training: Loss: 1.8151, Accuracy: 0.4688\nBatch number: 284, Training: Loss: 1.7381, Accuracy: 0.4688\nBatch number: 285, Training: Loss: 1.6002, Accuracy: 0.5625\nBatch number: 286, Training: Loss: 1.9683, Accuracy: 0.3125\nBatch number: 287, Training: Loss: 1.4252, Accuracy: 0.5312\nBatch number: 288, Training: Loss: 2.0353, Accuracy: 0.3750\nBatch number: 289, Training: Loss: 1.9464, Accuracy: 0.3438\nBatch number: 290, Training: Loss: 2.0691, Accuracy: 0.2812\nBatch number: 291, Training: Loss: 1.8676, Accuracy: 0.3750\nBatch number: 292, Training: Loss: 1.4442, Accuracy: 0.5000\nBatch number: 293, Training: Loss: 1.7410, Accuracy: 0.5000\nBatch number: 294, Training: Loss: 1.6439, Accuracy: 0.4062\nBatch number: 295, Training: Loss: 1.9761, Accuracy: 0.3750\nBatch number: 296, Training: Loss: 2.2090, Accuracy: 0.2500\nBatch number: 297, Training: Loss: 1.9263, Accuracy: 0.3750\nBatch number: 298, Training: Loss: 1.5265, Accuracy: 0.4688\nBatch number: 299, Training: Loss: 2.1550, Accuracy: 0.3125\nBatch number: 300, Training: Loss: 1.8064, Accuracy: 0.4375\nBatch number: 301, Training: Loss: 2.2925, Accuracy: 0.2812\nBatch number: 302, Training: Loss: 2.2291, Accuracy: 0.2812\nBatch number: 303, Training: Loss: 1.8571, Accuracy: 0.4375\nBatch number: 304, Training: Loss: 2.1287, Accuracy: 0.2500\nBatch number: 305, Training: Loss: 1.5250, Accuracy: 0.3438\nBatch number: 306, Training: Loss: 1.6396, Accuracy: 0.3750\nBatch number: 307, Training: Loss: 1.9077, Accuracy: 0.3438\nBatch number: 308, Training: Loss: 1.7987, Accuracy: 0.4375\nBatch number: 309, Training: Loss: 1.9149, Accuracy: 0.3750\nBatch number: 310, Training: Loss: 2.3166, Accuracy: 0.4375\nBatch number: 311, Training: Loss: 2.2791, Accuracy: 0.3438\nBatch number: 312, Training: Loss: 1.8240, Accuracy: 0.3750\nBatch number: 313, Training: Loss: 2.0652, Accuracy: 0.4375\nBatch number: 314, Training: Loss: 1.8154, Accuracy: 0.4062\nBatch number: 315, Training: Loss: 1.6938, Accuracy: 0.4688\nBatch number: 316, Training: Loss: 1.6248, Accuracy: 0.5312\nBatch number: 317, Training: Loss: 2.1881, Accuracy: 0.2812\nBatch number: 318, Training: Loss: 1.7848, Accuracy: 0.4375\nBatch number: 319, Training: Loss: 1.6927, Accuracy: 0.4062\nBatch number: 320, Training: Loss: 2.0528, Accuracy: 0.4062\nBatch number: 321, Training: Loss: 1.7607, Accuracy: 0.4062\nBatch number: 322, Training: Loss: 2.0208, Accuracy: 0.4375\nBatch number: 323, Training: Loss: 2.1060, Accuracy: 0.3750\nBatch number: 324, Training: Loss: 1.9396, Accuracy: 0.3125\nBatch number: 325, Training: Loss: 1.7946, Accuracy: 0.4062\nBatch number: 326, Training: Loss: 1.6090, Accuracy: 0.5625\nBatch number: 327, Training: Loss: 1.7174, Accuracy: 0.3125\nBatch number: 328, Training: Loss: 1.7585, Accuracy: 0.3438\nBatch number: 329, Training: Loss: 1.7387, Accuracy: 0.3438\nBatch number: 330, Training: Loss: 1.7000, Accuracy: 0.5312\nBatch number: 331, Training: Loss: 2.0054, Accuracy: 0.4062\nBatch number: 332, Training: Loss: 1.6857, Accuracy: 0.4062\nBatch number: 333, Training: Loss: 2.4108, Accuracy: 0.3125\nBatch number: 334, Training: Loss: 1.5808, Accuracy: 0.5625\nBatch number: 335, Training: Loss: 2.1725, Accuracy: 0.2812\nBatch number: 336, Training: Loss: 1.7692, Accuracy: 0.4375\nBatch number: 337, Training: Loss: 1.6080, Accuracy: 0.5000\nBatch number: 338, Training: Loss: 1.9011, Accuracy: 0.3438\nBatch number: 339, Training: Loss: 1.3913, Accuracy: 0.5000\nBatch number: 340, Training: Loss: 1.8150, Accuracy: 0.4062\nBatch number: 341, Training: Loss: 1.6059, Accuracy: 0.4688\nBatch number: 342, Training: Loss: 1.7729, Accuracy: 0.4062\nBatch number: 343, Training: Loss: 1.7347, Accuracy: 0.4062\nBatch number: 344, Training: Loss: 2.1924, Accuracy: 0.2812\nBatch number: 345, Training: Loss: 2.0505, Accuracy: 0.3750\nBatch number: 346, Training: Loss: 1.5266, Accuracy: 0.4688\nBatch number: 347, Training: Loss: 1.6756, Accuracy: 0.4375\nBatch number: 348, Training: Loss: 1.5598, Accuracy: 0.4375\nBatch number: 349, Training: Loss: 2.1012, Accuracy: 0.3750\nBatch number: 350, Training: Loss: 1.8634, Accuracy: 0.3750\nBatch number: 351, Training: Loss: 1.4569, Accuracy: 0.4688\nBatch number: 352, Training: Loss: 2.0911, Accuracy: 0.5000\nBatch number: 353, Training: Loss: 1.9982, Accuracy: 0.4688\nBatch number: 354, Training: Loss: 1.8915, Accuracy: 0.4062\nBatch number: 355, Training: Loss: 1.6354, Accuracy: 0.3438\nBatch number: 356, Training: Loss: 1.8835, Accuracy: 0.4375\nBatch number: 357, Training: Loss: 1.8982, Accuracy: 0.3438\nBatch number: 358, Training: Loss: 1.5317, Accuracy: 0.5938\nBatch number: 359, Training: Loss: 1.9227, Accuracy: 0.3438\nBatch number: 360, Training: Loss: 1.8906, Accuracy: 0.4062\nBatch number: 361, Training: Loss: 1.8415, Accuracy: 0.5000\nBatch number: 362, Training: Loss: 1.8865, Accuracy: 0.3125\nBatch number: 363, Training: Loss: 1.7096, Accuracy: 0.4688\nBatch number: 364, Training: Loss: 1.5619, Accuracy: 0.5312\nBatch number: 365, Training: Loss: 1.5550, Accuracy: 0.4688\nBatch number: 366, Training: Loss: 1.7825, Accuracy: 0.3125\nBatch number: 367, Training: Loss: 1.7172, Accuracy: 0.4375\nBatch number: 368, Training: Loss: 1.3886, Accuracy: 0.5625\nBatch number: 369, Training: Loss: 1.5487, Accuracy: 0.5625\nBatch number: 370, Training: Loss: 1.7179, Accuracy: 0.4062\nBatch number: 371, Training: Loss: 2.2532, Accuracy: 0.2812\nBatch number: 372, Training: Loss: 1.4726, Accuracy: 0.5625\nBatch number: 373, Training: Loss: 1.5573, Accuracy: 0.5000\nBatch number: 374, Training: Loss: 2.3186, Accuracy: 0.3438\nBatch number: 375, Training: Loss: 2.1370, Accuracy: 0.4062\nBatch number: 376, Training: Loss: 1.8445, Accuracy: 0.3750\nBatch number: 377, Training: Loss: 1.6884, Accuracy: 0.4688\nBatch number: 378, Training: Loss: 1.4872, Accuracy: 0.4062\nBatch number: 379, Training: Loss: 1.5289, Accuracy: 0.4688\nBatch number: 380, Training: Loss: 1.7937, Accuracy: 0.3750\nBatch number: 381, Training: Loss: 1.4304, Accuracy: 0.5625\nBatch number: 382, Training: Loss: 2.2738, Accuracy: 0.2812\nBatch number: 383, Training: Loss: 1.6555, Accuracy: 0.3750\nBatch number: 384, Training: Loss: 1.4567, Accuracy: 0.5938\nBatch number: 385, Training: Loss: 1.6361, Accuracy: 0.5000\nBatch number: 386, Training: Loss: 1.3552, Accuracy: 0.5000\nBatch number: 387, Training: Loss: 2.0366, Accuracy: 0.2812\nBatch number: 388, Training: Loss: 1.9885, Accuracy: 0.3438\nBatch number: 389, Training: Loss: 1.5818, Accuracy: 0.5625\nBatch number: 390, Training: Loss: 1.4754, Accuracy: 0.5312\nBatch number: 391, Training: Loss: 1.8324, Accuracy: 0.4375\nBatch number: 392, Training: Loss: 1.9494, Accuracy: 0.3125\nBatch number: 393, Training: Loss: 1.4742, Accuracy: 0.4688\nBatch number: 394, Training: Loss: 1.8896, Accuracy: 0.3750\nBatch number: 395, Training: Loss: 1.9609, Accuracy: 0.4062\nBatch number: 396, Training: Loss: 1.5021, Accuracy: 0.6250\nBatch number: 397, Training: Loss: 1.7292, Accuracy: 0.3750\nBatch number: 398, Training: Loss: 1.7615, Accuracy: 0.4062\nBatch number: 399, Training: Loss: 1.1830, Accuracy: 0.6562\nBatch number: 400, Training: Loss: 1.9132, Accuracy: 0.4688\nBatch number: 401, Training: Loss: 2.4191, Accuracy: 0.1250\nBatch number: 402, Training: Loss: 1.9024, Accuracy: 0.3750\nBatch number: 403, Training: Loss: 1.5767, Accuracy: 0.4375\nBatch number: 404, Training: Loss: 2.0449, Accuracy: 0.2812\nBatch number: 405, Training: Loss: 1.9526, Accuracy: 0.2812\nBatch number: 406, Training: Loss: 1.7419, Accuracy: 0.4688\nBatch number: 407, Training: Loss: 1.5651, Accuracy: 0.4688\nBatch number: 408, Training: Loss: 1.3419, Accuracy: 0.5312\nBatch number: 409, Training: Loss: 1.3046, Accuracy: 0.6250\nBatch number: 410, Training: Loss: 1.3688, Accuracy: 0.5000\nBatch number: 411, Training: Loss: 1.5107, Accuracy: 0.4062\nBatch number: 412, Training: Loss: 1.5428, Accuracy: 0.5312\nBatch number: 413, Training: Loss: 1.4252, Accuracy: 0.5625\nBatch number: 414, Training: Loss: 1.3569, Accuracy: 0.5312\nBatch number: 415, Training: Loss: 1.7526, Accuracy: 0.4062\nBatch number: 416, Training: Loss: 1.5582, Accuracy: 0.4688\n\n*** WARNING: skipped 712780 bytes of output ***\n\nBatch number: 253, Training: Loss: 0.3344, Accuracy: 0.8438\nBatch number: 254, Training: Loss: 0.1091, Accuracy: 1.0000\nBatch number: 255, Training: Loss: 0.2632, Accuracy: 0.8750\nBatch number: 256, Training: Loss: 0.3340, Accuracy: 0.8438\nBatch number: 257, Training: Loss: 0.2042, Accuracy: 0.9062\nBatch number: 258, Training: Loss: 0.5474, Accuracy: 0.8125\nBatch number: 259, Training: Loss: 0.1008, Accuracy: 0.9688\nBatch number: 260, Training: Loss: 0.3184, Accuracy: 0.8750\nBatch number: 261, Training: Loss: 0.2298, Accuracy: 0.9375\nBatch number: 262, Training: Loss: 0.2647, Accuracy: 0.9062\nBatch number: 263, Training: Loss: 0.2018, Accuracy: 0.9375\nBatch number: 264, Training: Loss: 0.4111, Accuracy: 0.8750\nBatch number: 265, Training: Loss: 0.3297, Accuracy: 0.8438\nBatch number: 266, Training: Loss: 0.2172, Accuracy: 0.9688\nBatch number: 267, Training: Loss: 0.5677, Accuracy: 0.8125\nBatch number: 268, Training: Loss: 0.3368, Accuracy: 0.8750\nBatch number: 269, Training: Loss: 0.3167, Accuracy: 0.8750\nBatch number: 270, Training: Loss: 0.4389, Accuracy: 0.8125\nBatch number: 271, Training: Loss: 0.3718, Accuracy: 0.8438\nBatch number: 272, Training: Loss: 0.3689, Accuracy: 0.8125\nBatch number: 273, Training: Loss: 0.4922, Accuracy: 0.8125\nBatch number: 274, Training: Loss: 0.6536, Accuracy: 0.8125\nBatch number: 275, Training: Loss: 0.8290, Accuracy: 0.7188\nBatch number: 276, Training: Loss: 0.4062, Accuracy: 0.8750\nBatch number: 277, Training: Loss: 0.3152, Accuracy: 0.8438\nBatch number: 278, Training: Loss: 0.3647, Accuracy: 0.8438\nBatch number: 279, Training: Loss: 0.2607, Accuracy: 0.8750\nBatch number: 280, Training: Loss: 0.3412, Accuracy: 0.9062\nBatch number: 281, Training: Loss: 0.5151, Accuracy: 0.8438\nBatch number: 282, Training: Loss: 0.1762, Accuracy: 0.9375\nBatch number: 283, Training: Loss: 0.3320, Accuracy: 0.9062\nBatch number: 284, Training: Loss: 0.3976, Accuracy: 0.9062\nBatch number: 285, Training: Loss: 0.5333, Accuracy: 0.7812\nBatch number: 286, Training: Loss: 0.2083, Accuracy: 0.9375\nBatch number: 287, Training: Loss: 0.3345, Accuracy: 0.8750\nBatch number: 288, Training: Loss: 0.3319, Accuracy: 0.8750\nBatch number: 289, Training: Loss: 0.3106, Accuracy: 0.8750\nBatch number: 290, Training: Loss: 0.6871, Accuracy: 0.7500\nBatch number: 291, Training: Loss: 0.4314, Accuracy: 0.8750\nBatch number: 292, Training: Loss: 0.2475, Accuracy: 0.9062\nBatch number: 293, Training: Loss: 0.4026, Accuracy: 0.9062\nBatch number: 294, Training: Loss: 0.3120, Accuracy: 0.8125\nBatch number: 295, Training: Loss: 0.3126, Accuracy: 0.8438\nBatch number: 296, Training: Loss: 0.4405, Accuracy: 0.8438\nBatch number: 297, Training: Loss: 0.3295, Accuracy: 0.9375\nBatch number: 298, Training: Loss: 0.4135, Accuracy: 0.6875\nBatch number: 299, Training: Loss: 0.3694, Accuracy: 0.8438\nBatch number: 300, Training: Loss: 0.6177, Accuracy: 0.7812\nBatch number: 301, Training: Loss: 0.4058, Accuracy: 0.8438\nBatch number: 302, Training: Loss: 0.2415, Accuracy: 0.8750\nBatch number: 303, Training: Loss: 0.2893, Accuracy: 0.8438\nBatch number: 304, Training: Loss: 0.3817, Accuracy: 0.8125\nBatch number: 305, Training: Loss: 0.2803, Accuracy: 0.8125\nBatch number: 306, Training: Loss: 0.4742, Accuracy: 0.8125\nBatch number: 307, Training: Loss: 0.3655, Accuracy: 0.8125\nBatch number: 308, Training: Loss: 0.2094, Accuracy: 0.9375\nBatch number: 309, Training: Loss: 0.1620, Accuracy: 0.9375\nBatch number: 310, Training: Loss: 0.2900, Accuracy: 0.8750\nBatch number: 311, Training: Loss: 0.3461, Accuracy: 0.8125\nBatch number: 312, Training: Loss: 0.2910, Accuracy: 0.8438\nBatch number: 313, Training: Loss: 0.3864, Accuracy: 0.8438\nBatch number: 314, Training: Loss: 0.2742, Accuracy: 0.8750\nBatch number: 315, Training: Loss: 0.1843, Accuracy: 0.9375\nBatch number: 316, Training: Loss: 0.4338, Accuracy: 0.7812\nBatch number: 317, Training: Loss: 0.3160, Accuracy: 0.8438\nBatch number: 318, Training: Loss: 0.3462, Accuracy: 0.8125\nBatch number: 319, Training: Loss: 0.1679, Accuracy: 0.9688\nBatch number: 320, Training: Loss: 0.4530, Accuracy: 0.8750\nBatch number: 321, Training: Loss: 0.6092, Accuracy: 0.6875\nBatch number: 322, Training: Loss: 0.4610, Accuracy: 0.8125\nBatch number: 323, Training: Loss: 0.4098, Accuracy: 0.8750\nBatch number: 324, Training: Loss: 0.4749, Accuracy: 0.7500\nBatch number: 325, Training: Loss: 0.3360, Accuracy: 0.8750\nBatch number: 326, Training: Loss: 0.4248, Accuracy: 0.8750\nBatch number: 327, Training: Loss: 0.1965, Accuracy: 0.9375\nBatch number: 328, Training: Loss: 0.4328, Accuracy: 0.8750\nBatch number: 329, Training: Loss: 0.2856, Accuracy: 0.9062\nBatch number: 330, Training: Loss: 0.2711, Accuracy: 0.9062\nBatch number: 331, Training: Loss: 0.5543, Accuracy: 0.7812\nBatch number: 332, Training: Loss: 0.6140, Accuracy: 0.7812\nBatch number: 333, Training: Loss: 0.4404, Accuracy: 0.8438\nBatch number: 334, Training: Loss: 0.3400, Accuracy: 0.8438\nBatch number: 335, Training: Loss: 0.2746, Accuracy: 0.9375\nBatch number: 336, Training: Loss: 0.3162, Accuracy: 0.9062\nBatch number: 337, Training: Loss: 0.2383, Accuracy: 0.9062\nBatch number: 338, Training: Loss: 0.2419, Accuracy: 0.8750\nBatch number: 339, Training: Loss: 0.3837, Accuracy: 0.8750\nBatch number: 340, Training: Loss: 0.2426, Accuracy: 0.9375\nBatch number: 341, Training: Loss: 0.2630, Accuracy: 0.9375\nBatch number: 342, Training: Loss: 0.2928, Accuracy: 0.9062\nBatch number: 343, Training: Loss: 0.3598, Accuracy: 0.8750\nBatch number: 344, Training: Loss: 0.2686, Accuracy: 0.9062\nBatch number: 345, Training: Loss: 0.5168, Accuracy: 0.7812\nBatch number: 346, Training: Loss: 0.1548, Accuracy: 0.9375\nBatch number: 347, Training: Loss: 0.6999, Accuracy: 0.7812\nBatch number: 348, Training: Loss: 0.3974, Accuracy: 0.8750\nBatch number: 349, Training: Loss: 0.2897, Accuracy: 0.8438\nBatch number: 350, Training: Loss: 0.2233, Accuracy: 0.8750\nBatch number: 351, Training: Loss: 0.4104, Accuracy: 0.8125\nBatch number: 352, Training: Loss: 0.4431, Accuracy: 0.8438\nBatch number: 353, Training: Loss: 0.2954, Accuracy: 0.9062\nBatch number: 354, Training: Loss: 0.5031, Accuracy: 0.8438\nBatch number: 355, Training: Loss: 0.2587, Accuracy: 0.8750\nBatch number: 356, Training: Loss: 0.4313, Accuracy: 0.8125\nBatch number: 357, Training: Loss: 0.5893, Accuracy: 0.7812\nBatch number: 358, Training: Loss: 0.3345, Accuracy: 0.8750\nBatch number: 359, Training: Loss: 0.3231, Accuracy: 0.9062\nBatch number: 360, Training: Loss: 0.4195, Accuracy: 0.8438\nBatch number: 361, Training: Loss: 0.1737, Accuracy: 0.9375\nBatch number: 362, Training: Loss: 0.0864, Accuracy: 1.0000\nBatch number: 363, Training: Loss: 0.3330, Accuracy: 0.9375\nBatch number: 364, Training: Loss: 0.1341, Accuracy: 0.9688\nBatch number: 365, Training: Loss: 0.3671, Accuracy: 0.8125\nBatch number: 366, Training: Loss: 0.3939, Accuracy: 0.7812\nBatch number: 367, Training: Loss: 0.2510, Accuracy: 0.9062\nBatch number: 368, Training: Loss: 0.3784, Accuracy: 0.8750\nBatch number: 369, Training: Loss: 0.6181, Accuracy: 0.6562\nBatch number: 370, Training: Loss: 0.3941, Accuracy: 0.8750\nBatch number: 371, Training: Loss: 0.4261, Accuracy: 0.8750\nBatch number: 372, Training: Loss: 0.4116, Accuracy: 0.9062\nBatch number: 373, Training: Loss: 0.3788, Accuracy: 0.8750\nBatch number: 374, Training: Loss: 0.4222, Accuracy: 0.7812\nBatch number: 375, Training: Loss: 0.3106, Accuracy: 0.8750\nBatch number: 376, Training: Loss: 0.4905, Accuracy: 0.8750\nBatch number: 377, Training: Loss: 0.3582, Accuracy: 0.9062\nBatch number: 378, Training: Loss: 0.4515, Accuracy: 0.8438\nBatch number: 379, Training: Loss: 0.3791, Accuracy: 0.7812\nBatch number: 380, Training: Loss: 0.4223, Accuracy: 0.8438\nBatch number: 381, Training: Loss: 0.7060, Accuracy: 0.7500\nBatch number: 382, Training: Loss: 0.3970, Accuracy: 0.8438\nBatch number: 383, Training: Loss: 0.3431, Accuracy: 0.8438\nBatch number: 384, Training: Loss: 0.2839, Accuracy: 0.8750\nBatch number: 385, Training: Loss: 0.5302, Accuracy: 0.8438\nBatch number: 386, Training: Loss: 0.2079, Accuracy: 0.9062\nBatch number: 387, Training: Loss: 0.4271, Accuracy: 0.8125\nBatch number: 388, Training: Loss: 0.3126, Accuracy: 0.8125\nBatch number: 389, Training: Loss: 0.1988, Accuracy: 0.9375\nBatch number: 390, Training: Loss: 0.3160, Accuracy: 0.8438\nBatch number: 391, Training: Loss: 0.4105, Accuracy: 0.7500\nBatch number: 392, Training: Loss: 0.4412, Accuracy: 0.8750\nBatch number: 393, Training: Loss: 0.3966, Accuracy: 0.8438\nBatch number: 394, Training: Loss: 0.2717, Accuracy: 0.8750\nBatch number: 395, Training: Loss: 0.4396, Accuracy: 0.8125\nBatch number: 396, Training: Loss: 0.0923, Accuracy: 1.0000\nBatch number: 397, Training: Loss: 0.2035, Accuracy: 0.9375\nBatch number: 398, Training: Loss: 0.2505, Accuracy: 0.8750\nBatch number: 399, Training: Loss: 0.3185, Accuracy: 0.8750\nBatch number: 400, Training: Loss: 0.2904, Accuracy: 0.8438\nBatch number: 401, Training: Loss: 0.3612, Accuracy: 0.8750\nBatch number: 402, Training: Loss: 0.1817, Accuracy: 0.9688\nBatch number: 403, Training: Loss: 0.4890, Accuracy: 0.7812\nBatch number: 404, Training: Loss: 0.2787, Accuracy: 0.8438\nBatch number: 405, Training: Loss: 0.0994, Accuracy: 0.9688\nBatch number: 406, Training: Loss: 0.5067, Accuracy: 0.8750\nBatch number: 407, Training: Loss: 0.6262, Accuracy: 0.7188\nBatch number: 408, Training: Loss: 0.4310, Accuracy: 0.8750\nBatch number: 409, Training: Loss: 0.3215, Accuracy: 0.8438\nBatch number: 410, Training: Loss: 0.2492, Accuracy: 0.9375\nBatch number: 411, Training: Loss: 0.3218, Accuracy: 0.8750\nBatch number: 412, Training: Loss: 0.5032, Accuracy: 0.8438\nBatch number: 413, Training: Loss: 0.5877, Accuracy: 0.7812\nBatch number: 414, Training: Loss: 0.1058, Accuracy: 0.9688\nBatch number: 415, Training: Loss: 0.3683, Accuracy: 0.8438\nBatch number: 416, Training: Loss: 0.5273, Accuracy: 0.8125\nBatch number: 417, Training: Loss: 0.2683, Accuracy: 0.9375\nBatch number: 418, Training: Loss: 0.4670, Accuracy: 0.8125\nBatch number: 419, Training: Loss: 0.7403, Accuracy: 0.8438\nBatch number: 420, Training: Loss: 0.3655, Accuracy: 0.8750\nBatch number: 421, Training: Loss: 0.2410, Accuracy: 0.9062\nBatch number: 422, Training: Loss: 0.2502, Accuracy: 0.8750\nBatch number: 423, Training: Loss: 0.3647, Accuracy: 0.8438\nBatch number: 424, Training: Loss: 0.1697, Accuracy: 0.9688\nBatch number: 425, Training: Loss: 0.5406, Accuracy: 0.8438\nBatch number: 426, Training: Loss: 0.6583, Accuracy: 0.8438\nBatch number: 427, Training: Loss: 0.3408, Accuracy: 0.8125\nBatch number: 428, Training: Loss: 0.2302, Accuracy: 0.8750\nBatch number: 429, Training: Loss: 0.2425, Accuracy: 0.8750\nBatch number: 430, Training: Loss: 0.5020, Accuracy: 0.9062\nBatch number: 431, Training: Loss: 0.2613, Accuracy: 0.9062\nBatch number: 432, Training: Loss: 0.2601, Accuracy: 0.9062\nBatch number: 433, Training: Loss: 0.3961, Accuracy: 0.8125\nBatch number: 434, Training: Loss: 0.2053, Accuracy: 0.9375\nBatch number: 435, Training: Loss: 0.1271, Accuracy: 0.9375\nBatch number: 436, Training: Loss: 0.3732, Accuracy: 0.8750\nBatch number: 437, Training: Loss: 0.4934, Accuracy: 0.6875\nBatch number: 438, Training: Loss: 0.2278, Accuracy: 0.8750\nBatch number: 439, Training: Loss: 0.2184, Accuracy: 0.8438\nBatch number: 440, Training: Loss: 0.3336, Accuracy: 0.8438\nBatch number: 441, Training: Loss: 0.4381, Accuracy: 0.8125\nBatch number: 442, Training: Loss: 0.4420, Accuracy: 0.8125\nBatch number: 443, Training: Loss: 0.2731, Accuracy: 0.9062\nBatch number: 444, Training: Loss: 0.2137, Accuracy: 0.9062\nBatch number: 445, Training: Loss: 0.1745, Accuracy: 0.9375\nBatch number: 446, Training: Loss: 0.3708, Accuracy: 0.8438\nBatch number: 447, Training: Loss: 0.3494, Accuracy: 0.8438\nBatch number: 448, Training: Loss: 0.3916, Accuracy: 0.8750\nBatch number: 449, Training: Loss: 0.5211, Accuracy: 0.7812\nBatch number: 450, Training: Loss: 0.2565, Accuracy: 0.9688\nBatch number: 451, Training: Loss: 0.4308, Accuracy: 0.8438\nBatch number: 452, Training: Loss: 0.1707, Accuracy: 0.9062\nBatch number: 453, Training: Loss: 0.2230, Accuracy: 0.9062\nBatch number: 454, Training: Loss: 0.3360, Accuracy: 0.8750\nBatch number: 455, Training: Loss: 0.2534, Accuracy: 0.8750\nBatch number: 456, Training: Loss: 0.1595, Accuracy: 0.9375\nBatch number: 457, Training: Loss: 0.2673, Accuracy: 0.9062\nBatch number: 458, Training: Loss: 0.2793, Accuracy: 0.9062\nBatch number: 459, Training: Loss: 0.4383, Accuracy: 0.8438\nBatch number: 460, Training: Loss: 0.4543, Accuracy: 0.8438\nBatch number: 461, Training: Loss: 0.4052, Accuracy: 0.7812\nBatch number: 462, Training: Loss: 0.3244, Accuracy: 0.8750\nBatch number: 463, Training: Loss: 0.3916, Accuracy: 0.9375\nBatch number: 464, Training: Loss: 0.8995, Accuracy: 0.8125\nBatch number: 465, Training: Loss: 0.3461, Accuracy: 0.8750\nBatch number: 466, Training: Loss: 0.3844, Accuracy: 0.8438\nBatch number: 467, Training: Loss: 0.3181, Accuracy: 0.9062\nBatch number: 468, Training: Loss: 0.5932, Accuracy: 0.9062\nBatch number: 469, Training: Loss: 0.3120, Accuracy: 0.8438\nBatch number: 470, Training: Loss: 0.3221, Accuracy: 0.8750\nBatch number: 471, Training: Loss: 0.3355, Accuracy: 0.8750\nBatch number: 472, Training: Loss: 0.2805, Accuracy: 0.9375\nBatch number: 473, Training: Loss: 0.1762, Accuracy: 0.9375\nBatch number: 474, Training: Loss: 0.3192, Accuracy: 0.8750\nBatch number: 475, Training: Loss: 0.3560, Accuracy: 0.8125\nBatch number: 476, Training: Loss: 0.4537, Accuracy: 0.8125\nBatch number: 477, Training: Loss: 0.2998, Accuracy: 0.9375\nBatch number: 478, Training: Loss: 0.3411, Accuracy: 0.9062\nBatch number: 479, Training: Loss: 0.3464, Accuracy: 0.9062\nBatch number: 480, Training: Loss: 0.3823, Accuracy: 0.8438\nBatch number: 481, Training: Loss: 0.3357, Accuracy: 0.8750\nBatch number: 482, Training: Loss: 0.2671, Accuracy: 0.9062\nBatch number: 483, Training: Loss: 0.5398, Accuracy: 0.7812\nBatch number: 484, Training: Loss: 0.2240, Accuracy: 0.9062\nBatch number: 485, Training: Loss: 0.3284, Accuracy: 0.8750\nBatch number: 486, Training: Loss: 0.4596, Accuracy: 0.8438\nBatch number: 487, Training: Loss: 0.1470, Accuracy: 0.9375\nBatch number: 488, Training: Loss: 0.2308, Accuracy: 0.8750\nBatch number: 489, Training: Loss: 0.2850, Accuracy: 0.8750\nBatch number: 490, Training: Loss: 0.2666, Accuracy: 0.8750\nBatch number: 491, Training: Loss: 0.1701, Accuracy: 0.9375\nBatch number: 492, Training: Loss: 0.2311, Accuracy: 0.8750\nBatch number: 493, Training: Loss: 0.2741, Accuracy: 0.8438\nBatch number: 494, Training: Loss: 0.5520, Accuracy: 0.8438\nBatch number: 495, Training: Loss: 0.5276, Accuracy: 0.9062\nBatch number: 496, Training: Loss: 0.4622, Accuracy: 0.8750\nBatch number: 497, Training: Loss: 0.3177, Accuracy: 0.8438\nBatch number: 498, Training: Loss: 0.4870, Accuracy: 0.8750\nBatch number: 499, Training: Loss: 0.3896, Accuracy: 0.7812\nBatch number: 500, Training: Loss: 0.3395, Accuracy: 0.8438\nBatch number: 501, Training: Loss: 0.3612, Accuracy: 0.9062\nBatch number: 502, Training: Loss: 0.4908, Accuracy: 0.7812\nBatch number: 503, Training: Loss: 0.4953, Accuracy: 0.9062\nBatch number: 504, Training: Loss: 0.4759, Accuracy: 0.8750\nBatch number: 505, Training: Loss: 0.3245, Accuracy: 0.8750\nBatch number: 506, Training: Loss: 0.4685, Accuracy: 0.8125\nBatch number: 507, Training: Loss: 0.2401, Accuracy: 0.9375\nBatch number: 508, Training: Loss: 0.1714, Accuracy: 0.9688\nBatch number: 509, Training: Loss: 0.4163, Accuracy: 0.8750\nBatch number: 510, Training: Loss: 0.1354, Accuracy: 0.9688\nBatch number: 511, Training: Loss: 0.5572, Accuracy: 0.8438\nBatch number: 512, Training: Loss: 0.2713, Accuracy: 0.9375\nBatch number: 513, Training: Loss: 0.4028, Accuracy: 0.8438\nBatch number: 514, Training: Loss: 0.1503, Accuracy: 0.9062\nBatch number: 515, Training: Loss: 0.2461, Accuracy: 0.9062\nBatch number: 516, Training: Loss: 0.1839, Accuracy: 0.9688\nBatch number: 517, Training: Loss: 0.4162, Accuracy: 0.9062\nBatch number: 518, Training: Loss: 0.6875, Accuracy: 0.8438\nBatch number: 519, Training: Loss: 0.1923, Accuracy: 0.9688\nBatch number: 520, Training: Loss: 0.3262, Accuracy: 0.8750\nBatch number: 521, Training: Loss: 0.3341, Accuracy: 0.8438\nBatch number: 522, Training: Loss: 0.2025, Accuracy: 0.9375\nBatch number: 523, Training: Loss: 0.4028, Accuracy: 0.7812\nBatch number: 524, Training: Loss: 0.2504, Accuracy: 0.9375\nBatch number: 525, Training: Loss: 0.3276, Accuracy: 0.9062\nBatch number: 526, Training: Loss: 0.3515, Accuracy: 0.9062\nBatch number: 527, Training: Loss: 0.4333, Accuracy: 0.8125\nBatch number: 528, Training: Loss: 0.2717, Accuracy: 0.9375\nBatch number: 529, Training: Loss: 0.3485, Accuracy: 0.8750\nBatch number: 530, Training: Loss: 0.1849, Accuracy: 0.9062\nBatch number: 531, Training: Loss: 0.2481, Accuracy: 0.8750\nBatch number: 532, Training: Loss: 0.1975, Accuracy: 0.8750\nBatch number: 533, Training: Loss: 0.2013, Accuracy: 0.9062\nBatch number: 534, Training: Loss: 0.4858, Accuracy: 0.7188\nBatch number: 535, Training: Loss: 0.4020, Accuracy: 0.8125\nBatch number: 536, Training: Loss: 0.6164, Accuracy: 0.8125\nBatch number: 537, Training: Loss: 0.0815, Accuracy: 1.0000\nBatch number: 538, Training: Loss: 0.2401, Accuracy: 0.9062\nBatch number: 539, Training: Loss: 0.2084, Accuracy: 0.8750\nBatch number: 540, Training: Loss: 0.5229, Accuracy: 0.8125\nBatch number: 541, Training: Loss: 0.4111, Accuracy: 0.8750\nBatch number: 542, Training: Loss: 0.2871, Accuracy: 0.8750\nBatch number: 543, Training: Loss: 0.2535, Accuracy: 0.9062\nBatch number: 544, Training: Loss: 0.2954, Accuracy: 0.8750\nBatch number: 545, Training: Loss: 0.6167, Accuracy: 0.8125\nBatch number: 546, Training: Loss: 0.5294, Accuracy: 0.8125\nBatch number: 547, Training: Loss: 0.2834, Accuracy: 0.9062\nBatch number: 548, Training: Loss: 0.9064, Accuracy: 0.8125\nBatch number: 549, Training: Loss: 0.6093, Accuracy: 0.8750\nBatch number: 550, Training: Loss: 0.4932, Accuracy: 0.8438\nBatch number: 551, Training: Loss: 0.3035, Accuracy: 0.9062\nBatch number: 552, Training: Loss: 0.1789, Accuracy: 0.9375\nBatch number: 553, Training: Loss: 0.3808, Accuracy: 0.8750\nBatch number: 554, Training: Loss: 0.8402, Accuracy: 0.6875\nBatch number: 555, Training: Loss: 0.5915, Accuracy: 0.7812\nBatch number: 556, Training: Loss: 0.3319, Accuracy: 0.8750\nBatch number: 557, Training: Loss: 0.2268, Accuracy: 0.9375\nBatch number: 558, Training: Loss: 0.3959, Accuracy: 0.8438\nBatch number: 559, Training: Loss: 0.3597, Accuracy: 0.9062\nBatch number: 560, Training: Loss: 0.5893, Accuracy: 0.7812\nBatch number: 561, Training: Loss: 0.5120, Accuracy: 0.7812\nBatch number: 562, Training: Loss: 0.4310, Accuracy: 0.8438\nBatch number: 563, Training: Loss: 0.4210, Accuracy: 0.7812\nBatch number: 564, Training: Loss: 0.2299, Accuracy: 0.9688\nBatch number: 565, Training: Loss: 0.5839, Accuracy: 0.8125\nBatch number: 566, Training: Loss: 0.1366, Accuracy: 0.9375\nBatch number: 567, Training: Loss: 0.3317, Accuracy: 0.8125\nBatch number: 568, Training: Loss: 0.3090, Accuracy: 0.8125\nBatch number: 569, Training: Loss: 0.3036, Accuracy: 0.8750\nBatch number: 570, Training: Loss: 0.6743, Accuracy: 0.7500\nBatch number: 571, Training: Loss: 0.4008, Accuracy: 0.8438\nBatch number: 572, Training: Loss: 0.3822, Accuracy: 0.8750\nBatch number: 573, Training: Loss: 0.3978, Accuracy: 0.8750\nBatch number: 574, Training: Loss: 0.2986, Accuracy: 0.9062\nBatch number: 575, Training: Loss: 0.4426, Accuracy: 0.8438\nBatch number: 576, Training: Loss: 0.3438, Accuracy: 0.8438\nBatch number: 577, Training: Loss: 0.3248, Accuracy: 0.8333\nValidation Batch number: 000, Validation: Loss: 0.4362, Accuracy: 0.8438\nValidation Batch number: 001, Validation: Loss: 0.6925, Accuracy: 0.6875\nValidation Batch number: 002, Validation: Loss: 0.2613, Accuracy: 0.8438\nValidation Batch number: 003, Validation: Loss: 0.2580, Accuracy: 0.9688\nValidation Batch number: 004, Validation: Loss: 0.2366, Accuracy: 0.8438\nValidation Batch number: 005, Validation: Loss: 0.1826, Accuracy: 0.9688\nValidation Batch number: 006, Validation: Loss: 1.5035, Accuracy: 0.8125\nValidation Batch number: 007, Validation: Loss: 0.4909, Accuracy: 0.7812\nValidation Batch number: 008, Validation: Loss: 0.8646, Accuracy: 0.7188\nValidation Batch number: 009, Validation: Loss: 0.5053, Accuracy: 0.8438\nValidation Batch number: 010, Validation: Loss: 0.2806, Accuracy: 0.9062\nValidation Batch number: 011, Validation: Loss: 0.3999, Accuracy: 0.8125\nValidation Batch number: 012, Validation: Loss: 0.3004, Accuracy: 0.8750\nValidation Batch number: 013, Validation: Loss: 0.1630, Accuracy: 0.9375\nValidation Batch number: 014, Validation: Loss: 0.3795, Accuracy: 0.8438\nValidation Batch number: 015, Validation: Loss: 0.1879, Accuracy: 0.9375\nValidation Batch number: 016, Validation: Loss: 0.3379, Accuracy: 0.9062\nValidation Batch number: 017, Validation: Loss: 0.5345, Accuracy: 0.7812\nValidation Batch number: 018, Validation: Loss: 0.2640, Accuracy: 0.8438\nValidation Batch number: 019, Validation: Loss: 0.6062, Accuracy: 0.8750\nValidation Batch number: 020, Validation: Loss: 0.2939, Accuracy: 0.9062\nValidation Batch number: 021, Validation: Loss: 0.2897, Accuracy: 0.8438\nValidation Batch number: 022, Validation: Loss: 0.4042, Accuracy: 0.8438\nValidation Batch number: 023, Validation: Loss: 0.3944, Accuracy: 0.8438\nValidation Batch number: 024, Validation: Loss: 0.3532, Accuracy: 0.8438\nValidation Batch number: 025, Validation: Loss: 0.3385, Accuracy: 0.8438\nValidation Batch number: 026, Validation: Loss: 0.4703, Accuracy: 0.6875\nValidation Batch number: 027, Validation: Loss: 0.3211, Accuracy: 0.7812\nValidation Batch number: 028, Validation: Loss: 0.5829, Accuracy: 0.7812\nValidation Batch number: 029, Validation: Loss: 0.2149, Accuracy: 0.9375\nValidation Batch number: 030, Validation: Loss: 0.4638, Accuracy: 0.8750\nValidation Batch number: 031, Validation: Loss: 0.1432, Accuracy: 0.9688\nValidation Batch number: 032, Validation: Loss: 0.1659, Accuracy: 0.9062\nValidation Batch number: 033, Validation: Loss: 0.3103, Accuracy: 0.8438\nValidation Batch number: 034, Validation: Loss: 0.3113, Accuracy: 0.8750\nValidation Batch number: 035, Validation: Loss: 0.2020, Accuracy: 0.9062\nValidation Batch number: 036, Validation: Loss: 0.2922, Accuracy: 0.9062\nValidation Batch number: 037, Validation: Loss: 0.2564, Accuracy: 0.9375\nValidation Batch number: 038, Validation: Loss: 0.3559, Accuracy: 0.9062\nValidation Batch number: 039, Validation: Loss: 0.1353, Accuracy: 0.9062\nValidation Batch number: 040, Validation: Loss: 0.4048, Accuracy: 0.9062\nValidation Batch number: 041, Validation: Loss: 0.5712, Accuracy: 0.8438\nValidation Batch number: 042, Validation: Loss: 0.3227, Accuracy: 0.8438\nValidation Batch number: 043, Validation: Loss: 0.2820, Accuracy: 0.9062\nValidation Batch number: 044, Validation: Loss: 0.2430, Accuracy: 0.8750\nValidation Batch number: 045, Validation: Loss: 0.3848, Accuracy: 0.7812\nValidation Batch number: 046, Validation: Loss: 0.4536, Accuracy: 0.8438\nValidation Batch number: 047, Validation: Loss: 0.2624, Accuracy: 0.9062\nValidation Batch number: 048, Validation: Loss: 0.2156, Accuracy: 0.9375\nValidation Batch number: 049, Validation: Loss: 0.2325, Accuracy: 0.9375\nValidation Batch number: 050, Validation: Loss: 0.2663, Accuracy: 0.8750\nValidation Batch number: 051, Validation: Loss: 0.1608, Accuracy: 0.9375\nValidation Batch number: 052, Validation: Loss: 0.6969, Accuracy: 0.6875\nValidation Batch number: 053, Validation: Loss: 0.3202, Accuracy: 0.8438\nValidation Batch number: 054, Validation: Loss: 0.8412, Accuracy: 0.8438\nValidation Batch number: 055, Validation: Loss: 0.3909, Accuracy: 0.8438\nValidation Batch number: 056, Validation: Loss: 0.7510, Accuracy: 0.7188\nValidation Batch number: 057, Validation: Loss: 0.4342, Accuracy: 0.8750\nValidation Batch number: 058, Validation: Loss: 0.4264, Accuracy: 0.8750\nValidation Batch number: 059, Validation: Loss: 0.5044, Accuracy: 0.7812\nValidation Batch number: 060, Validation: Loss: 0.2693, Accuracy: 0.8438\nValidation Batch number: 061, Validation: Loss: 0.3603, Accuracy: 0.9062\nValidation Batch number: 062, Validation: Loss: 0.4705, Accuracy: 0.8125\nValidation Batch number: 063, Validation: Loss: 0.3399, Accuracy: 0.9062\nValidation Batch number: 064, Validation: Loss: 0.3000, Accuracy: 0.9062\nValidation Batch number: 065, Validation: Loss: 0.3015, Accuracy: 0.8750\nValidation Batch number: 066, Validation: Loss: 0.6362, Accuracy: 0.7188\nValidation Batch number: 067, Validation: Loss: 0.2600, Accuracy: 0.9062\nValidation Batch number: 068, Validation: Loss: 0.2883, Accuracy: 0.8750\nValidation Batch number: 069, Validation: Loss: 0.2917, Accuracy: 0.8750\nValidation Batch number: 070, Validation: Loss: 0.5819, Accuracy: 0.8125\nValidation Batch number: 071, Validation: Loss: 0.2796, Accuracy: 0.8750\nValidation Batch number: 072, Validation: Loss: 0.6068, Accuracy: 0.5714\nEpoch : 019, Training: Loss: 0.3505, Accuracy: 86.8985%, \n\t\tValidation : Loss : 0.3858, Accuracy: 85.6772%, Time: 517.7310s\n</div>"]}}],"execution_count":13},{"cell_type":"code","source":["\nmodel_save_path = '/dbfs/mnt/RAW/FILES/SYNAPSE/POC/UDACITY/PRODUCT_IMAGES_CLASSIFIER/checkpoint-74.pth'\nhistory_save_path = '/dbfs/mnt/RAW/FILES/SYNAPSE/POC/UDACITY/PRODUCT_IMAGES_CLASSIFIER/history.pth'\n\n#history = torch.load(history_save_path)\n\n#OLD VERSIOn\ntrained_model = torch.load(model_save_path)\n\n##NEW VERSION\n#checkpoint = torch.load(PATH)\n#trained_model.load_state_dict(checkpoint['model_state_dict'])\n#optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n#epoch = checkpoint['epoch']\n#loss = checkpoint['loss']\n"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["history = np.array(history)\nplt.plot(history[:,0:2])\nplt.legend(['Training Loss', 'Validation Loss'])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.ylim(0,1)\nplt.savefig('/dbfs/mnt/RAW/FILES/SYNAPSE/POC/UDACITY/PRODUCT_IMAGES_CLASSIFIER/model_loss_curve.png')\ndisplay(plt.show())"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["plt.plot(history[:,2:4])\nplt.legend(['Training Accuracy', 'Validation Accuracy'])\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.ylim(0,1)\nplt.savefig('/dbfs/mnt/RAW/FILES/SYNAPSE/POC/UDACITY/PRODUCT_IMAGES_CLASSIFIER/accuracy_curve.png')\ndisplay(plt.show())"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["def computeTestSetAccuracy(model, loss_criterion):\n  '''\n  Function to compute the accuracy on the test set\n  Parameters\n      :param model: Model to test\n      :param loss_criterion: Loss Criterion to minimize\n  '''\n\n  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n  test_acc = 0.0\n  test_loss = 0.0\n\n  # Validation - No gradient tracking needed\n  with torch.no_grad():\n\n      # Set to evaluation mode\n      model.eval()\n\n      # Validation loop\n      for j, (inputs, labels) in enumerate(test_data_loader):\n          inputs = inputs.to(device)\n          labels = labels.to(device)\n\n          # Forward pass - compute outputs on input data using the model\n          outputs = model(inputs)\n\n          # Compute loss\n          loss = loss_criterion(outputs, labels)\n\n          # Compute the total loss for the batch and add it to valid_loss\n          test_loss += loss.item() * inputs.size(0)\n\n          # Calculate validation accuracy\n          ret, predictions = torch.max(outputs.data, 1)\n          correct_counts = predictions.eq(labels.data.view_as(predictions))\n\n          # Convert correct_counts to float and then compute the mean\n          acc = torch.mean(correct_counts.type(torch.FloatTensor))\n\n          # Compute total accuracy in the whole batch and add to valid_acc\n          test_acc += acc.item() * inputs.size(0)\n\n          print(\"Test Batch number: {:03d}, Test: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n\n  # Find average test loss and test accuracy\n  avg_test_loss = test_loss/test_data_size \n  avg_test_acc = test_acc/test_data_size\n\n  print(\"Test accuracy : \" + str(avg_test_acc))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":17},{"cell_type":"code","source":["computeTestSetAccuracy(model, loss_criterion)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Test Batch number: 000, Test: Loss: 0.3229, Accuracy: 0.8750\nTest Batch number: 001, Test: Loss: 0.4761, Accuracy: 0.7812\nTest Batch number: 002, Test: Loss: 0.2331, Accuracy: 0.8750\nTest Batch number: 003, Test: Loss: 0.4625, Accuracy: 0.8750\nTest Batch number: 004, Test: Loss: 1.1806, Accuracy: 0.8438\nTest Batch number: 005, Test: Loss: 0.6082, Accuracy: 0.7500\nTest Batch number: 006, Test: Loss: 0.3498, Accuracy: 0.8125\nTest Batch number: 007, Test: Loss: 0.4394, Accuracy: 0.7812\nTest Batch number: 008, Test: Loss: 0.3101, Accuracy: 0.8438\nTest Batch number: 009, Test: Loss: 0.1923, Accuracy: 0.9062\nTest Batch number: 010, Test: Loss: 0.4970, Accuracy: 0.8750\nTest Batch number: 011, Test: Loss: 0.2824, Accuracy: 0.8750\nTest Batch number: 012, Test: Loss: 0.2427, Accuracy: 0.9375\nTest Batch number: 013, Test: Loss: 0.3759, Accuracy: 0.9062\nTest Batch number: 014, Test: Loss: 0.7127, Accuracy: 0.7812\nTest Batch number: 015, Test: Loss: 0.4438, Accuracy: 0.7812\nTest Batch number: 016, Test: Loss: 0.4426, Accuracy: 0.8438\nTest Batch number: 017, Test: Loss: 0.4252, Accuracy: 0.9062\nTest Batch number: 018, Test: Loss: 0.2396, Accuracy: 0.9062\nTest Batch number: 019, Test: Loss: 0.3073, Accuracy: 0.8750\nTest Batch number: 020, Test: Loss: 0.5928, Accuracy: 0.8438\nTest Batch number: 021, Test: Loss: 0.4181, Accuracy: 0.8125\nTest Batch number: 022, Test: Loss: 0.5247, Accuracy: 0.8125\nTest Batch number: 023, Test: Loss: 0.9181, Accuracy: 0.6875\nTest Batch number: 024, Test: Loss: 0.4494, Accuracy: 0.9062\nTest Batch number: 025, Test: Loss: 0.4094, Accuracy: 0.8125\nTest Batch number: 026, Test: Loss: 0.1707, Accuracy: 0.9062\nTest Batch number: 027, Test: Loss: 0.4867, Accuracy: 0.8125\nTest Batch number: 028, Test: Loss: 0.2976, Accuracy: 0.8750\nTest Batch number: 029, Test: Loss: 0.5290, Accuracy: 0.8750\nTest Batch number: 030, Test: Loss: 0.1748, Accuracy: 0.9062\nTest Batch number: 031, Test: Loss: 0.1777, Accuracy: 0.9375\nTest Batch number: 032, Test: Loss: 0.5118, Accuracy: 0.8438\nTest Batch number: 033, Test: Loss: 0.2959, Accuracy: 0.8438\nTest Batch number: 034, Test: Loss: 0.3488, Accuracy: 0.8125\nTest Batch number: 035, Test: Loss: 0.5682, Accuracy: 0.8125\nTest Batch number: 036, Test: Loss: 0.2419, Accuracy: 0.9375\nTest Batch number: 037, Test: Loss: 0.6518, Accuracy: 0.8750\nTest Batch number: 038, Test: Loss: 0.2590, Accuracy: 0.8750\nTest Batch number: 039, Test: Loss: 0.5618, Accuracy: 0.7812\nTest Batch number: 040, Test: Loss: 0.3224, Accuracy: 0.8750\nTest Batch number: 041, Test: Loss: 0.4807, Accuracy: 0.8750\nTest Batch number: 042, Test: Loss: 0.3348, Accuracy: 0.8125\nTest Batch number: 043, Test: Loss: 0.2916, Accuracy: 0.8750\nTest Batch number: 044, Test: Loss: 0.5562, Accuracy: 0.8125\nTest Batch number: 045, Test: Loss: 0.3109, Accuracy: 0.8750\nTest Batch number: 046, Test: Loss: 0.4989, Accuracy: 0.9062\nTest Batch number: 047, Test: Loss: 0.3024, Accuracy: 0.9062\nTest Batch number: 048, Test: Loss: 0.4859, Accuracy: 0.7812\nTest Batch number: 049, Test: Loss: 0.3760, Accuracy: 0.8438\nTest Batch number: 050, Test: Loss: 0.3148, Accuracy: 0.9062\nTest Batch number: 051, Test: Loss: 0.3889, Accuracy: 0.8438\nTest Batch number: 052, Test: Loss: 0.5145, Accuracy: 0.8438\nTest Batch number: 053, Test: Loss: 0.3762, Accuracy: 0.8750\nTest Batch number: 054, Test: Loss: 0.5472, Accuracy: 0.7188\nTest Batch number: 055, Test: Loss: 0.3098, Accuracy: 0.9062\nTest Batch number: 056, Test: Loss: 0.2262, Accuracy: 0.8438\nTest Batch number: 057, Test: Loss: 0.5344, Accuracy: 0.7500\nTest Batch number: 058, Test: Loss: 0.4077, Accuracy: 0.8125\nTest Batch number: 059, Test: Loss: 0.5234, Accuracy: 0.7812\nTest Batch number: 060, Test: Loss: 0.6588, Accuracy: 0.9062\nTest Batch number: 061, Test: Loss: 0.5096, Accuracy: 0.7812\nTest Batch number: 062, Test: Loss: 0.4138, Accuracy: 0.7812\nTest Batch number: 063, Test: Loss: 0.2916, Accuracy: 0.9062\nTest Batch number: 064, Test: Loss: 0.5050, Accuracy: 0.8125\nTest Batch number: 065, Test: Loss: 0.2081, Accuracy: 0.9375\nTest Batch number: 066, Test: Loss: 0.5342, Accuracy: 0.8438\nTest Batch number: 067, Test: Loss: 0.5219, Accuracy: 0.8125\nTest Batch number: 068, Test: Loss: 0.2678, Accuracy: 0.9375\nTest Batch number: 069, Test: Loss: 0.4308, Accuracy: 0.7500\nTest Batch number: 070, Test: Loss: 0.4118, Accuracy: 0.8750\nTest Batch number: 071, Test: Loss: 0.6594, Accuracy: 0.7812\nTest Batch number: 072, Test: Loss: 0.3125, Accuracy: 0.9062\nTest Batch number: 073, Test: Loss: 0.5285, Accuracy: 0.7857\nTest accuracy : 0.8472340424517367\n</div>"]}}],"execution_count":18},{"cell_type":"code","source":["def predict(model, test_image_name):\n    '''\n    Function to predict the class of a single test image\n    Parameters\n        :param model: Model to test\n        :param test_image_name: Test image\n\n    '''\n    \n    transform = image_transforms['test']\n\n    test_image = Image.open(test_image_name)\n    plt.imshow(test_image)\n    \n    test_image_tensor = transform(test_image)\n\n    if torch.cuda.is_available():\n        test_image_tensor = test_image_tensor.view(1, 3, 224, 224).cuda()\n    else:\n        test_image_tensor = test_image_tensor.view(1, 3, 224, 224)\n    \n    with torch.no_grad():\n        model.eval()\n        # Model outputs log probabilities\n        out = model(test_image_tensor)\n        ps = torch.exp(out)\n        topk, topclass = ps.topk(3, dim=1)\n        for i in range(3):\n            #print(\"Prediction\", i+1, \":\", idx_to_class[topclass.cpu().numpy()[0][i]], \", Score: \", topk.cpu().numpy()[0][i])\n            print(\"Prediction\", i+1, \":\", topclass.cpu().numpy()[0][i], \", Score: \", topk.cpu().numpy()[0][i])"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["predict(trained_model, image_dir_valid + '1011995/133.jpg')"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":21}],"metadata":{"name":"Model Building","notebookId":1280775501282941},"nbformat":4,"nbformat_minor":0}
